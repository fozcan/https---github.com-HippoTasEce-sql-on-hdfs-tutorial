\documentclass{vldb}
%\usepackage{graphicx,amsmath,amssymb,balance}
%\usepackage{enumitem}
%\usepackage[ruled,vlined]{algorithm2e}
%\usepackage[usenames,dvipsnames]{color}
%\usepackage{ifthen}
%\usepackage{hyperref} % package used for the \autoref* command
%\hypersetup{draft} % don't create hyperlinks
%\usepackage{soul}
%\usepackage{mathtools}
%\usepackage{tikz}
%\usetikzlibrary{arrows}
\usepackage{balance}
\usepackage{url}
\begin{document}
\title{Tutorial: SQL-on-Hadoop Systems}

\numberofauthors{4}

% 1st author
\author{
\alignauthor
Daniel Abadi\\
\affaddr{Yale University}\\
\email{daniel.abadi@yale.edu}
\alignauthor 
\newcommand{\spshiv }{\hspace{-12mm}}
\spshiv Shivnath Babu\\
\spshiv \affaddr{Duke University}
\spshiv \email{shivnath@cs.duke.edu} 
\alignauthor
\newcommand{\spfatma}{\hspace{-15mm}}
\spfatma Fatma {\"O}zcan\\
\spfatma \affaddr{IBM Research}
\spfatma \email{fozcan@us.ibm.com}
\alignauthor
\newcommand{\spippo}{\hspace{-12mm}}
\spippo  Ippokratis Pandis\\
\spippo \affaddr{Cloudera}
\spippo \email{ippokratis@cloudera.com}
}
\maketitle
% ----------------
% --- Abstract ---
% ----------------
%\begin{abstract}
%\end{abstract}
\section{Introduction}
Enterprises are using Hadoop as a central data repository for all their data coming from various sources,  including operational systems, social media and the web, sensors and smart devices, as well as their applications. SQL processing over Hadoop data in particular gained significant traction over the couple of years, as many enterprise data management tools rely on SQL,
and many enterprise users are familiar and comfortable with it. As a result, the number of SQL-on-Hadoop systems have increased significantly. 

%Over the last decade, the database field has witnessed significant major innovations and changes in enterprise data platforms. First came the wave of noSQL, which provide high scale-out systems, although at the expense of strong ACID transactions and declarative SQL processing. On the analytics side, Hadoop emerged as the platform for all analytics needs of the enterprise. Although Hadoop started with just MapReduce and HDFS, it evolved into a multi-framework system, which supports not only MapReduce but also Spark, Tez, SQL, and others. The driver that enables all these frameworks to work in a single cluster environment is the shared file system, Hadoop File System (HDFS).Given the popularity of SQL in the enterprise, it was soon evident that SQL processing over HDFS data is critical in this new emerging enterprise data platform.

In this tutorial, we use the term SQL-on-Hadoop to refer to systems that provide some level of declarative SQL(-like) processing over HDFS and noSQL data sources, using architectures that include computational or storage engines compatible with Apache Hadoop. We classify SQL-on-Hadoop systems into five categories:

\begin{itemize}
\item {{\bf Native systems with big data run-times:} These are systems that utilize a big data run-time such as Tez, or Spark to execute SQL queries. This category includes systems such as Hive and Spark SQL.}
\item{ {\bf Native systems with MPP architectures:} These are systems that employ an MPP database architecture that use stand-alone long running processing to execute SQL queries over HDFS data. This category includes systems such as Impala, Presto, Drill, and Big SQL}
\item{{\bf Database hybrids:} These are systems that combine a relational SQL engine with Hadoop run-time. This category includes systems such as Hadapt, HAWQ, and Vortex.}
\item{ {\bf Database connectors:} This category includes the extensions provided by major database vendors to reach out to Hadoop and ingest HDFS data. }
\item{ {\bf noSQL/Hbase specific variants:} These are systems that provide a declarative SQL layer on top of noSQL systems, HBase in particular. The systems in this category are Phoenix, and Splice Machine. Note that these systems focus on OLTP-like workloads, whereas all other categories address OLAP-like analytics}
\end{itemize}

An important aspect of SQL-on-Hadoop, which is orthogonal to all these categories, is the various HDFS file formats used to store the data. While text is an important format for in-situ SQL analysis, and ELT processing, various columnar formats have been proposed to provide fast analytics query processing. In this tutorial, we will first introduce these file formats, then we will discuss the characteristics, the pros and cons of each category in detail. In particular, we will examine their architectural differences, and cover in-depth their approach to indexing, updates, query optimization, as well as new algorithms they introduce in a 3-hour tutorial.

\section{HDFS File Formats}

\section{Category 1: Native systems with big data run-times}

Hive \cite{hive} was the first SQL-on-Hadoop offering that provided an SQL-like query language, called HiveQL, and used MapReduce run-time to execute queries. Hive compiled HiveQL queries into a series of map reduce jobs. Hive provided an interpreter that ran inside the map and reduce functions and executed various relational operations, including projection, selection, joins, and aggregations. Data movement and grouping were implemented using the MapReduce shuffle operator.

As SQL-on-Hadoop gained popularity, MapReduce based run-time did not provide the required response times, due to the high latency in launching map reduce jobs. To address this issue, Hive moved to a different run-time, Tez, which can run DAGs as a single job, reducing the latency in launching jobs. With Tez, a complex HiveQL query can be compiled into a single Tez job. Hive-on-Tez also provides pipelining data without writing into disk between stages of a Tez job.

Spark is a fast, general purpose cluster computing engine that is compatible with Hadoop data and addresses the shortcoming of MapReduce. There are three different systems that use Spark as their run-time for SQL processing: Shark \cite{sharksigmod13}, Hive on Spark \cite{hiveOnSpark}, and Spark SQL \cite{sparkSQL}. Shark replaces the MapReduce run-time of Hive \cite{hive} with Spark run-time. It is still uses the Hive compiler, and interfaces but generates Spark jobs. Shark introduced an in-memory columnar data organization that allowed for fast execution of SQL queries. Hive on Spark is another attempt at replacing MapReduce/Tez based run-times of Hive with Spark run-time, but still relies on the Hive interpreter. Finally, Spark SQL uses the Shark \cite{sharksigmod13} run-time but replaces the HiveQL and compiler with a new compiler for SQL and uses the catalyst query optimizer.

We will discuss each of these systems in detail, comparing their compilers and run-times, and discussing the pros and cons of each approach.


\section{Category 2: Native systems with MPP architectures}

As it became clear that the latency of launching jobs for each query is too expensive for interactive SQL query processing, there was a shift to shared-nothing database architectures for SQL processing over Hadoop data. In this group, a long-running process co-exists with dataNodes on each node in the cluster, and continuously answers SQL queries. The systems in this category implement their own run-times and data movements between their long-running processes. 

After discussing the general characteristics of the systems in this category, we will examine 3 representatives systems in some detail: Impala, Big SQL, and Drill. 

Cloudera Impala \cite{Kornacker+15} an open-source, fully-integrated MPP SQL query engine. Impala's goal is to combine the familiar SQL support and multi-user performance of a traditional analytic database with the scalability and flexibility of Apache Hadoop. Unlike other systems (often forks of Postgres), Impala is a brand-new engine. It is written from the ground up in C++ and Java, and employs uses LLVM to generate code at runtime in order to speed up frequently executed code paths. It maintains Hadoop's flexibility by utilizing standard components (HDFS, HBase, Metastore, YARN, Sentry) and is able to read the majority of the widely-used file formats (e.g. Parquet, Avro, RCFile). To reduce latency, such as that incurred from utilizing MapReduce or by reading data remotely, Impala implements a distributed architecture based on daemon processes that are responsible for all aspects of query execution and that run on the same machines as the rest of the Hadoop infrastructure. The result is performance that is on par or exceeds that of commercial MPP analytic DBMSs, depending on the particular workload.

IBM Big SQL \cite{bigsql} leverages IBM's state-of-the-art relational database technology, without imposing any database structures or restrictions, and processes standard SQL queries over HDFS data, supporting all common Hadoop file formats, including text and sequence files, as well as Parquet and ORC files. Big SQL does not introduce any propriety data format. Big SQL 3.0 shares the same catalog and table definitions with Hive using the Hive Metastore. Database workers read HDFS data directly and execute relational operations. Big SQL exploits sophisticated query rewrite transformations \cite{pirahesh96, winmagic} that are targeted for complex nested decision support queries. It uses sophisticated data statistics and a cost-based optimizer to choose the best query execution plan. In traditional shared-nothing databases, each worker owns a partition of the data. Big SQL deviates from this basic model and introduces a scheduler service that assigns HDFS blocks to database workers for processing on a query by query basis. The scheduler identifies where the HDFS blocks are and decides which database workers to include in the query plan. The assignment is done dynamically at run-time to accommodate failures: scheduler uses the workers that are currently up and running. This also allows elasticity. If a new node is added to the database cluster, it can be considered immediately by the scheduler for the new queries. Similarly, if a node crashes or the cluster is scaled down, the scheduler immediately detects this change and chooses database workers for future queries accordingly.

Apache Drill \cite{drill} is an open-source project which aims at providing SQL-like declarative processing over self-describing semi-structured data. Its focus is on analyzing data without emposing a fixed schema or creating tables in a catalog like Hive MetaStore. It runs queries over files and HBase tables, and discovers data when reading input data. For every fixed chunk of data, it discovers its schema, creates an in-memory columnar representation, and generates specific code for processing. As such, it can accommodate data chunks with varying schemas. Drill provides a columnar execution engine that delays materialization as late as possible.

\section{Category 3: Database hybrids}

\section{Category 4: Database connectors}

All major database vendors provide some sort of connector to HDFS data. Some of these are just external table interfaces that allows to read HDFS data directly from the database. Early connectors from IBM DB2 \cite{ibm-sigmod2011}, Oracle, and TeraData are examples.
All these solutions provide parallel data ingestion capabilities, but do not pushdown any processing to the Hadoop side. All HDFS data is ingested and processed in the database.

Oracle Big Data SQL \cite{bigdataSQL}, and TeraData QueryGrid \cite{queryGrid} are more advanced solutions which are capable of delegating some query processing to Hadoop. Big Data SQL provides stand alone processes that run on the Hadoop platform and can execute projection and selection before sending the data to the Oracle database. TeraData QueryGrid leverages Hive on the Hadoop side to pushdown selection and projections. While these solutions limit the amount of data that needs to be moved into the database, the query processing, especially the joins and aggregations are still executed inside the database.

\section{Category 5: noSQL/HBase specific variants}

An important category of SQL-on-Hadoop includes systems that provide some level of SQL support over HBase data. HBase provides auto-sharding and fail over technology for scaling tables across multiple servers. It also enables updates, running on top of the HDFS, which itself does not support updates. HBase scales out to petabytes of data easily over a cluster of commodity hardware. It has become very popular to store IoT (Internet of Things) data, web, mobile and social data, as well as machine-generated data, as HBase supports high data ingestion rates that is typical for these data sets. In this category, we will examine two representative systems; namely Splice Machine \cite{splice} and Phoenix \cite{phoenix}. 

Splice Machine \cite{splice} provides SQL support over HBase data using Apache Derby, targeting both operational as well as analytical workloads. It replaces the storage system and run-time of Derby with HBase and Hadoop. Splice Machine leverages the Derby compiler stack to generate execution plans that access HBase servers. Splice Machine adapts the Derby optimizer and planner to leverage HBase capabilities. 


\bibliographystyle{abbrv}
\bibliography{references}
\balance
\end{document}
